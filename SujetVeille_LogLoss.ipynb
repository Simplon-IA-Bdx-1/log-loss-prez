{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sujet de veille\n",
    "## Log Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Théorie et petit exemple..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Log Loss et Python !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Log-loss comme évaluation Kaggle ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Un super secret pour gagner toutes les compètes Kaggle......... 8-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "Log Loss est la **métrique basée sur la probabilité de la classe positive** pour les problèmes de **classification**.\n",
    "\n",
    "Elle répond à cette question :\n",
    "\n",
    "### A quel point le modèle est-il certain que sa prédiction appartient à la classe positive/négative ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La théorie\n",
    "C'est difficile d'interpréter une valeur isolée de log-loss, en revanche elle est très utile pour **comparer plusieurs modèles** entre eux.\n",
    "\n",
    "Pour chaque problème donné, **plus la valeur de log loss est faible, meilleure est la prédiction.**\n",
    "\n",
    "Log Loss s'apparente à la **Likelihood Function** (fonction de ressemblance). \n",
    "\n",
    "### En fait, Log Loss est **-1 fois le log de la likelihood function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Petit exemple \n",
    "\n",
    "Un modèle prédit les probabilités de vente suivantes pour 3 maisons : **[0.8, 0.4, 0.1]**. Les 2 premières sont vendues, pas la dernière. L'output réel serait donc : **[1, 1, 0]**.\n",
    "\n",
    "Calculons la likelihood fonction pour chacune des maisons.\n",
    "\n",
    "La première maison est vendue, estimation à 80% sûre. La likelihood est donc de **0.8** après une prédiction.\n",
    "\n",
    "La deuxième maison est vendue, cette fois estimation 40%. La règle de probabilité veut que la proba de plusieurs éléments indépendents soit le produit de chacune de leur proba. Donc ça nous donne une likelihood combinée de **0.8 * 0.4 = 0.32**.\n",
    "\n",
    "La troisième maison n'est pas vendue. Le modèle a dit qu'elle avait 10% de chances de se vendre, donc **90% de ne PAS se vendre**. On multiplie donc le résultat précédent de **0.32 par 0.9 = 0.288**.\n",
    "\n",
    "On pourrait itérer comme ça pour toutes les prédictions : c'est la Likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From Likelihood to Log Loss\n",
    "\n",
    "Chaque **probabilité est entre 0 et 1**. En multipliant des nombres si petits entre eux on atteint des nombres de plus en plus minuscules, difficiles à calculer.\n",
    "\n",
    "On a donc choisi d'utiliser **le log de Likelihood**. On multiplie le résultat par -1 pour garder à l'esprit que des scores bas sont meilleurs.\n",
    "\n",
    "**A suivre, la formule mathématique et un aperçu de sa courbe.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![formule](images_veille/formule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\frac{1}{n} \\sum \\log$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![9](images_veille/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Loss et Python !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Définition de la fonction logloss\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def logloss(true_label, predicted, eps=1e-15):\n",
    "#     p = np.clip(predicted, eps, 1 - eps)\n",
    "    p = predicted\n",
    "    if true_label == 1:\n",
    "        return -np.log(p) \n",
    "    else: \n",
    "        return -np.log(1 - p)\n",
    "\n",
    "logloss(1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21616187468057912"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La librairie Sklearn nous permet d'importer cette fonction\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],[[.1, .9], [.9, .1], [.8, .2], [.35, .65]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Keras propose aussi une estimation calculée après chaque époque, appelée categorical_crossentropy. \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Un autre exemple: \n",
    "\n",
    "### binary cross-entropy / log loss as your loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [-2.2 -1.4 -0.8  0.2  0.4  0.8  1.2  2.2  2.9  4.6]\n",
      "y = [0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "p(y) = [0.19 0.33 0.47 0.7  0.74 0.81 0.86 0.94 0.97 0.99]\n",
      "Log Loss / Cross Entropy = 0.3329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([-2.2, -1.4, -.8, .2, .4, .8, 1.2, 2.2, 2.9, 4.6])\n",
    "y = np.array([0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]) # Labels ; 1 positive class\n",
    "\n",
    "logr = LogisticRegression(solver='lbfgs')\n",
    "logr.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "y_pred = logr.predict_proba(x.reshape(-1, 1))[:, 1].ravel()\n",
    "loss = log_loss(y, y_pred)\n",
    "\n",
    "print('x = {}'.format(x))\n",
    "print('y = {}'.format(y))\n",
    "print('p(y) = {}'.format(np.round(y_pred, 2)))\n",
    "print('Log Loss / Cross Entropy = {:.4f}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Given our feature x, we need to predict its label: red or green\n",
    "\n",
    "![1](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Splitting the data in negative class & positive class\n",
    "\n",
    "![2](2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now, let’s train a Logistic Regression to classify our points.**\n",
    "\n",
    "**The fitted regression is a sigmoid curve representing the probability of a point being green for any given x.**\n",
    "\n",
    "**It looks like this:**\n",
    "\n",
    "![3](3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Probabilities of classifying points in the POSITIVE class correctly\n",
    "\n",
    "![4](4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Probabilities of classifying points in the NEGATIVE class correctly\n",
    "\n",
    "![5](5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### All probabilities put together\n",
    "\n",
    "![6](6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### These probabilities are all we need, so, let’s get rid of the x axis and bring the bars next to each other:\n",
    "\n",
    "![7](7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Repositioning\n",
    "\n",
    "![8](8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The plot below gives us a clear picture —as the predicted probability of the true class gets closer to zero,\n",
    "#### the loss increases exponentially:\n",
    "\n",
    "![9](9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let’s take the (negative) log of the probabilities — these are the corresponding losses of each and every point.\n",
    "#### Finally, we compute the mean of all these losses.\n",
    "#### We have successfully computed the binary cross-entropy / log loss of this toy example. It is 0.3329.\n",
    "\n",
    "\n",
    "\n",
    "   ![10](10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss / Cross Entropy = 0.3329\n"
     ]
    }
   ],
   "source": [
    "#Et pour rappel, sklearn nous donnait :\n",
    "print('Log Loss / Cross Entropy = {:.4f}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Le log loss comme évaluation Kaggle ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## + Google Cloud & NCAA®️ ML Competition 2018-Men's\n",
    "\n",
    "Compétition organisée par Google Cloud et la NCAA (National Collegiate Athletic Association)\n",
    "\n",
    "Objectif : Prédire les résultats des matchs du tournoi de 1ere division de basket Universitaire appelé tournoi 'March Madness'(la folie de mars).\n",
    "\n",
    "Prévision : pour 68 équipes dans l'année du tournoi\n",
    "        - Phase 1 : 2014-2017 : 68 * 67/2 * 4 an = 9112 prévisions \n",
    "        - Phase 2 : 2018 : 2278 prévisions.\n",
    "   \n",
    "Evaluation kaggle : log-loss\n",
    "\n",
    "Vainqueur March Madness de 2018 : Villanova (Université de Pensilvanie)\n",
    "[Feuille des matchs](https://fr.wikipedia.org/wiki/Championnat_NCAA_de_basket-ball_2018)\n",
    "\n",
    "### Top 1 Kaggle (25.000 USD) : mtodisco10 avec un score de 0.53194\n",
    "\n",
    "[Lien de la compétition](https://www.kaggle.com/c/mens-machine-learning-competition-2018/overview/description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## + PLAsTiCC Astronomical Classification\n",
    "\n",
    "Compétition organisée par la LSST Corporation et l'Université de Toronto.\n",
    "\n",
    "Objectif : Classer entre plusieurs catégories des images issues de l'espace qui évoluent avec le temps, le tout avec des datasets devenant de plus en plus volumineux, jusqu'au type de données récupérées par le **Large Synoptic Survey Telescope** (LSST), nouveau téléscope à la pointe de la technologie.\n",
    "\n",
    "L'université se sert ensuite des prédictions des Kagglers pour calibrer ses propres modèles pour mieux interpréter les images !\n",
    "\n",
    "Evaluation kaggle : weighted multi-class logarithmic loss\n",
    "\n",
    "### Top 1 Kaggle (12.000 USD) : Kyle Boone avec un score de 0.68503\n",
    "\n",
    "[Lien de la compétition](https://www.kaggle.com/c/PLAsTiCC-2018/overview/description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![kaggle2](kaggle_2.png)\n",
    "\n",
    "Ici on ne soumet pas qu'une probabilité mais la probabilité d'appartenance **à chacune des classes** ! La somme de ces probas faisant bien sûr 1.\n",
    "\n",
    "Par exemple :\n",
    "\n",
    "  *object_id,class_6,class_15,class_16,class_42,class_52,class_53*\n",
    "  \n",
    "  13,0,0.1,0,0.1,0,0.8\n",
    "  \n",
    "  25,0,0.05,0.15,0,0,0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Un petit récap...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Log Loss est une métrique à utiliser pour les problèmes de classification. Elle peut s'appliquer aux problèmes binaires comme de classification multiple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Elle sera toujours entre 0 et 1 et on cherche à ce qu'elle soit la plus petite possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Elle punit fortement les grosses erreurs, permettant de différencier un modèle qui se trompe de beaucoup par rapport à un autre qui serait plus confiant dans ses prédicitons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Sklearn et Keras fournissent des outils pour la calculer aisément."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Et enfin ce que vous attendiez tous, l'astuce pour rafler la mise à chaque fois sur Kaggle !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The use of the logarithm provides extreme punishments for **being both confident and wrong**. \n",
    "\n",
    "In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score, it would be much better to keep our probabilities between 0.05–0.95 so that we are never very sure about our prediction. \n",
    "\n",
    "**In this case, we won’t see the massive growth of an error function.**\n",
    "\n",
    "![courbe](trick.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Trick: np.clip\n",
    "\n",
    "### We need to change all values that less than 0.025 to be equal to 0.025 and all values that are more than 0.975 to be equal to 0.975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict(data)\n",
    "# => [0.3, 0.0021, 0.5, 0.98...]\n",
    "\n",
    "y_pred_clipped = np.clip(y_pred, 0.025, 0.975)\n",
    "# => [0.3, 0.025, 0.5, 0.975...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## On peut même faire :\n",
    "```\n",
    "    submission['prediction'] = model.predict(data).clip(0.025, 0.975)\n",
    "    submission.to_csv('clipped_sumbission.csv', index=False)\n",
    "```\n",
    "\n",
    "## A vous la gloire et les dollars !!!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
